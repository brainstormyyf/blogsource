---
title: 项目详细方案
date: 2024-02-28T23:29:44+08:00
draft: false
type: docs
linkTitle: 项目详细方案
nav_weight: 2
authors:
  - YYF
---
<!--more-->


**赛题编号及名称： 【A05】视频人像分割算法【万兴科技】团队编号：** **2103655**

**完成时间： 2022 年 4 月**

## 第一章 前言

近年来，随着深度学习的不断发展，计算机视觉应用火热。第十三届中国大学生服务外包-创新创业大赛上，在【A05】给出了视频人像分割算法的赛题，来自中南大学的计院打工人队基于已有技术和可实现的模型，实现了在不依赖网络的情况下，向程序输入一个待分割的视频，程序自动输出分割后的人像前景结果的既定需求。并且团队在此基础上精益求精，不断改进模型性能，在应用到赛题方提供的测试集时，获得良好的效果。接下来，我们将会从产品概述，产品设计，产品的测试与实现等维度对于相关成果进行阐述。

## 第二章 作品概述

### 2.1 背景及研发意义

计算机视觉因为深度学习的发展,近年来应用非常的火热。效能和速度也不断的精进, 过去从云端的服务,逐步脱离云服务发展到 PC 端边缘运算,到这几年再往移动端运算发展。尽管移动端运算能力越来越强大,轻量级模型的架构设计也是一个很重要的因素。

图像语义分割算法逐步成熟，计算机硬件的计算能力越来越强大,算法的落地装置从云服务器端转移到边缘计算端,不管是在视频特效应用或是视频会议软件虚拟背景相关应用则越来越常见。图像语义分割中的视频人像分割算法,主要注重的是精细度和速度，尤其是在 PC 或是移动端上更有其挑战性。

此赛题专注在快速的人像分割算法,主要目标为设计一个抠图算法在不使用 GPU 的环境下运行,在精细度、速度和模型大小上取得平衡。

而在产品的研发意义方面，我们发现在当下，随着直播等在线多媒体行业的发展，高效的视频人像分割算法应用前景十分广阔。在一些例如在线会议，新闻播报等需要实 时交互的场合，替换背景往往需要在主持人身后添加绿屏道具。此时若能利用视频人像分割算法，便可在无需道具的条件下替换背景，从而降低替换背景的成本。另外，视频人像分割算法也可以被用于处理部分视频素材，对其中的人像或人像外的背景进行提取，方便创作者利用素材，创作出大众喜闻乐见的娱乐视频作品。

### 2.2 作品简介

团队以收集开源数据和自主建立数据集的方式获得数据、建立人像分割算法模型。并完成模型训练、优化、工程化等工作，最终产出一个可执行程序，针对赛题方给出的 20 个视频测试集进行人像分割。

并且，程序提供 PC 端可执行程序入口，实现了在不依赖网络的情况下，向程序输入一个待分割的视频，程序自动输出分割后的人像前景结果与视频 Mask；向程序输入待分割的图片，程序自动输出分割后的图片 Mask。

### 2.3 模型特色

<span style="background:#fff88f">（1）连贯提取视频人像，抑制了闪烁、残影问题</span>

其主要解决方法在 RNN 的应用，视频是有关联图像的序列，在处理视频时充分考虑了视频前几帧与本帧在时间和空间维度上的相关性，减少了残影与闪烁问题；

我们发现,当前主流的视频处理算法把视频的每一帧当作独立的图像来处理，输出都是只考虑前一个输入的影响而不考虑其它时刻输入的影响,这样仅对于单个图像的识别具有较好的效果。但是, 这种算法没有考虑到视频本身的特性，不能利用到视频时间序列给出的信息。对于解决一些与时间先后有关的问题, 比如视频的下一时刻的预测等,这些算法的表现就不尽如人意。RNN 是一种特殊的神经网络结构,具体的表现形式为网络会对前面的信息进行记忆并应用于当前输出的计算中，即隐藏层之间的节点不再无连接而是有连接的，并且隐藏层的输入不仅包括输入层的输出还包括上一时刻隐藏层的输出。

<span style="background:#fff88f">（2）对高清视频的快速处理</span>

首先通过下采样的操作，使我们的算法在较低像素维度上进行运算处理，最后再通过 DGF（deep guided filter）上采样的处理,把抠图 mask 恢复到原始分辨率，这极大加速了对于高分辨率视频的处理。其中，对 1080P 视频的处理，在不使用 GPU 的情况下，也能达到 20fps。

<span style="background:#fff88f">（3）“发丝级”的精准识别</span>

模型在多个维度上进行学习，使其自身更好地理解了人像本身的特征，能够识别高矮胖瘦的人，能够识别全身像、半身像，进而提高了预测的精准度，实现了“发丝级”的抠图；

我们通过应用运动和时间增强来增加数据的多样性。运动增强包括仿射平移、缩放、旋转、纯粹、亮度、饱和度、对比度、色调、噪声和模糊。增强还向图像数据集添加了人工运动。时间增强包括剪辑反转、速度变化、随机暂停和跳帧。此外还有其他的离散增强也被应用到了模型中去。

![151](https://gitee.com/yao_yi_feng/fighouse/raw/master/img/%E8%A7%86%E9%A2%91%E4%BA%BA%E5%83%8F%E5%88%86%E5%89%B2/202402290001231.webp?width=500#center)

![151_mask](https://gitee.com/yao_yi_feng/fighouse/raw/master/img/%E8%A7%86%E9%A2%91%E4%BA%BA%E5%83%8F%E5%88%86%E5%89%B2/202402290024893.webp?width=500#center)

## 第三章 系统设计

### 3.1 总体设计
#### 3.1.1 什么是人像抠图、人像分割

对于一张图 I， 我们感兴趣的人像部分称为前景 F，其余部分为背景 B，则图像 I 可以视为 F 与 B 的加权融合：I = alpha * F + (1 - alpha) * B，而抠图任务就是找到合适的权重 alpha。值得一提的是，如图，查看抠图 ground truth 可以看到，alpha 是 [0, 1]之间的连续值，可以理解为像素属于前景的概率。而在人像分割任务中，alpha只能取 0 或 1，本质上是分类任务，而抠图是回归任务。虽然人像抠图与人像分割有细微的不同，但是我们认为二者是联系紧密的，人像抠图可以看作是人像分割的边缘细化处理，人像分割也可以帮助人像抠图的训练过程。

- 抠图 ground truth 

![IMG_256](https://gitee.com/yao_yi_feng/fighouse/raw/master/img/%E8%A7%86%E9%A2%91%E4%BA%BA%E5%83%8F%E5%88%86%E5%89%B2/202402290001632.webp?width=500#center)

- 分割 ground truth

![IMG_256](https://gitee.com/yao_yi_feng/fighouse/raw/master/img/%E8%A7%86%E9%A2%91%E4%BA%BA%E5%83%8F%E5%88%86%E5%89%B2/202402290001065.webp?width=500#center)

#### 3.1.2 视频人像抠图

视频人像分割实现的抠图部分是实现从输入帧中预测 alpha 和 foreground 的过程。每一帧 I 可以看作是前景 F 和背景 B 通过α系数的线性组合:I=αA +（1-α）B。通过提取α，我们获得一个图像的背景与前景，达到视频人像分割的效果。

在处理视频人像抠图时我们应充分考虑到视频的特殊性，视频是一系列帧的序列，序列包含了时间上的信息，如果我们把每一帧当作独立的图像来做处理，则没有利用到时间上的信息，提取出的人像存在不连贯的问题，体现在提取视频上则是闪烁和残影问题。

为此在现有理论下，我们采用的模型使用了 RNN 的方法（RNN 网络具有时间序列学习的先验知识），使模型可以学习到时间序列的信息，以此可以提高视频抠图性能。首先，它允许预测更连贯的结果，因为模型可以看到多个帧和它自己的预测。这显著减少了闪烁并提高了感知质量。其次，时间信息可以提高抠图的鲁棒性。第三，时间信息允许模型随着时间的推移更多地了解背景。因此，我们采用的是一种循环架构来利用时间信息。并且，在应用时，该模型不需要任何 trimap 输入。

不仅如此，模型采用一种新的训练策略来满足抠图和语义分割目标。大多数现有方法都是在合成消光数据集上训练的。样本通常看起来很假，并阻止网络泛化为真实图像。我们认为人类抠图任务与人类分割任务密切相关。

### 3.2 模型设计

#### 3.2.1 模型架构

![](https://gitee.com/yao_yi_feng/fighouse/raw/master/img/%E8%A7%86%E9%A2%91%E4%BA%BA%E5%83%8F%E5%88%86%E5%89%B2/202402290003142.webp?width=700#center)

模型流程图展示

我们的架构由一个提取单个帧特征的编码器（encoder）、一个聚合时间信息的循环解码器（recurrent decoder）和一个用于高分辨率上采样的深度引导滤波器模块（Deep Guided Filter module）组成。上图显示了我们的模型架构。其中，我们的精度格式为 FP32，参数量（Params）为 3.75MB，计算量（Flops）为 0.775GFlops，模型大小 14.2MB。

模型的算法总的来说就是一个将输入帧通过下采样得到不同分辨率下的尺度特征，再一步步通过卷积的上采样形式结合矫正算法使得输入帧重新得到高分辨率的前景以及 alpha 的过程。

#### 3.2.2 编码器

在编码器部分，我们采用 MobileNetV3-Large 作为我们的主干， 然后采用 MobileNetV3 提出的用于语义分割任务的 LR-ASPP 模块。此外，MobileNetV3 的最后一个块使用扩张卷积而没有下采样步幅。编码器模块对单个帧进行操作，并为循环解码器提取 1/2 、1/4 、1/8 和 1/16 尺度的特征。由此来适应十六个通道的工作。上述即为 Encoder 覆盖括号内的内容。

![](https://gitee.com/yao_yi_feng/fighouse/raw/master/img/%E8%A7%86%E9%A2%91%E4%BA%BA%E5%83%8F%E5%88%86%E5%89%B2/202402290003930.webp?width=600#center)

![IMG_256](https://gitee.com/yao_yi_feng/fighouse/raw/master/img/%E8%A7%86%E9%A2%91%E4%BA%BA%E5%83%8F%E5%88%86%E5%89%B2/202402290003891.webp?width=500#center)

#### 3.2.3 循环解码器

在解码器部分，鉴于循环机制可以在连续的视频流上自行学习要保留和忘记哪些信息以及循环机制自适应地保留长期和短期时间信息的能力非常可观，我们的解码器采用多个尺度的 ConvGRU 来聚合时间信息。

解码器由瓶颈块（bottle-neck block）、上采样块（upsampling block）和输出块（out put）组成。通过 split 和 concatation 在一半的通道上应用 ConvGRU 有效且高效。这种设计有助于 ConvGRU 专注于聚合时间信息，而另一个被 split 的分支则转发特定于当前帧的空间特征。除了最后一个投影使用 1 × 1 内核之外，所有卷积都使用3 × 3 内核。

![图示, 文本  描述已自动生成](https://gitee.com/yao_yi_feng/fighouse/raw/master/img/%E8%A7%86%E9%A2%91%E4%BA%BA%E5%83%8F%E5%88%86%E5%89%B2/202402290003685.webp?width=700#center)

<span style="background:#fff88f">MobileNetV3</span> 通过结合硬件感知网络架构搜索(NAS)和 NetAdapt 算法，对移动端的 cpu进行调优，然后通过新的架构改进对其进行改进。MobileNetV1 使用高效的深度可分离卷积来替代传统卷积。深度可分离卷积通过将空间滤波与特征生成机制分离，有效的分解了传统卷积。MobileNetV2 引入线性 bottleneck 与反向 ResNet 结构来利用问题的低秩性使得层结构更加高效。而对于 MobileNetV3，则使用这些层的组合来构建更加高效的模块。因此获得更加高效和便捷的运算模块。

<span style="background:#fff88f">LR-ASPP</span> 主要将MobileNetV3 的最后一个 block 中不同 resolution 的信息，通过与 SE- net，skip-connection 相结合，实现移动端的图像分割。

<span style="background:#fff88f">ConvGRU</span> 是一种循环神经网络，是 LSTM 的变种，在 LSTM 神经网络的基础上优化了 cell结构减少了参数，加快了训练速度。LSTM 的 cell 结构分为：遗忘门，决定前一层传递过来的多大程度被遗忘掉；输入门，控制当前计算的新状态多大程度更新到记忆细胞中；输出门，控制当前输出有多大程度取决于当前的记忆单元；记忆单元，可以看出细胞状态是有权重、输入、上一层的隐含层输入、上一层的记忆单元状态、输入门综合计算得到的；而本层的隐含层状态则是由输出门与记忆细胞状态决定。GRU 摒弃了 LSTM 中的记忆单元，并将输入门和遗忘门结合成了更新门（update gate），用来决定有多少迁移一层的状态要更新当前神经元中，因此更容易用来聚合时间信息。

<span style="background:#fff88f">bottle-neck block</span> 在 LR-ASPP 模块之后，瓶颈块以 1/16 个特征尺度运行。ConvGRU层通过 split 和 concat 仅在一半的通道上运行。这显着减少了参数和计算量，因为 ConvGRU 在计算上是可扩展的。

<span style="background:#fff88f">上采样块</span>以 1/8 、 1/4 和 1/2 的比例重复。首先，它将前一个块的双线性上采样输出、编码器相应比例的特征图和通过重复的 2×2 平均池化下采样的输入图像连接起来。然后，应用卷积，然后是 Batch Normalization（深度网络中经常用到的加速神经网络训练，加速收敛速度及稳定性的算法）和 ReLU（使用激活函数来加入非线性因素，提高模型的表达能力）来执行特征合并和通道缩减。最后，通过 split 和 concat 将 ConvGRU 应用于一半的通道。

<span style="background:#fff88f">Output</span> 使用常规卷积来细化结果。它首先连接输入图像和前一个块的双线性上采样输出。然后它使用 2 次重复卷积、批量归一化和 ReLU 堆栈来产生最终的隐藏特征。最后，将特征投影到输出。

#### 3.2.4 上采样模块

我们采用深度引导滤波器（DGF）进行高分辨率预测。在处理高分辨率视频时，我们将输入帧通过下采样（downsample）得到一个因子 s，然后再通过编码器-解码器网络。然后将低分辨率 alpha、foreground、final hidden features 以及高分辨率输入帧提供给 DGF 模块以产生高分辨率 alpha 和前景。DGF 模块是可选模块，如果要处理的视频分辨率较低，则编码器解码器网络可以独立运行。

：DGF 为深度引导滤波器，是在引导滤波器的基础上加入了可学习的参数之后使其对于联合上采样问题的解决具有较好的处理能力。DFG 可以解决 Joint Upsampling 任务，给它一个高分辨率的输入图像和一个低分辨率输出图像，算法可以输出高分辨率的输出图像，它的细节和边缘与原始给定图像相似。

## 第四章 工程化实现

### 4.1 深度学习的工作流程

下图为深度学习的一般工作流程。在得到训练好的 pytorch 模型后，最终要应用到实际社会生产中，模型的部署是必不可少的一步。直接使用基于 python 的深度学习框架在 CPU 部署模型，模型运算效率较低。在追求低延时的场景下，这样的方式几乎不可取。那么在这种情况下 ONNX 就是一个大杀器了。

![](https://gitee.com/yao_yi_feng/fighouse/raw/master/img/%E8%A7%86%E9%A2%91%E4%BA%BA%E5%83%8F%E5%88%86%E5%89%B2/202402290004067.webp?width=700#center)

### 4.2 什么是 ONNX

优化用于推理（或模型评分）的深度学习模型非常困难，因为需要调整模型和推理库，充分利用硬件功能。 如果想要在不同类型的平台（云、CPU/GPU 等）上获得最佳性能，实现起来会异常困难，因为每个平台都有不同的功能和特性。 如果模型来自需要在各种平台上运行的多种框架，会极大增加复杂性。 优化框架和硬件的所有不同组合非常耗时。这就需要一种解决方案，在首选框架中训练一次后能在云上的任意位置运行。此时 ONNX 便派上了用场。

ONNX 是一种用于表示机器学习模型的开放格式。 ONNX 定义了一组通用运算符——机器学习和深度学习模型的构建块——以及一种通用文件格式，使 AI 开发人员能够 使用具有各种框架、工具、运行时和编译器的模型。许多框架（包括 TensorFlow、PyTorch、 SciKit-Learn、Keras、Chainer、MXNet、MATLAB 和 SparkML）中的模型都可以导出或转换为标准 ONNX 格式。 模型采用 ONNX 格式后，可在各种平台和设备上运行。

### 4.3 基于 ONNX 进行 C++工程化部署：

基于 ONNX 进行的 C++模型部署在技术上主要分为四个步骤：构建模型、导出为 ONNX的格式、使用 ONNX 进行推理、导出为 C++框架。

#### 4.3.1 将模型导出为 ONNX 格式

在形式上，我们在完成基于 Pytorch 的模型构建之后，使用的 Pytorch 自带的模块torch.onnx 将模型转换到 ONNX 格式

#### 4.3.2 实现 Python 版本的 ONNXRuntime 推理

在进行 C++实现之前，我们先实现了一份 python 的推理。如果要更好地将该项目转换成 C++工程，需要考虑工程依赖的问题。因为，Python 中一些非常便于使用的第三方

依赖库，通常情况下，难以在 C++中找到同等的实现。所以，为了便于实现 C++和 Python的工程效果对齐，我们编写了一个只依赖于 OpenCV、numpy 和 onnxruntime 的 python推理。

#### 4.3.3 实现 C++版本的 ONNXRuntime 推理

相关代码附上百度网盘链接：链接：[百度网盘 请输入提取码 (baidu.com)](https://pan.baidu.com/share/init?surl=EqnA1IKseTyA9YTyZkzavA&pwd=p29o) 提取码：p29o

### 4.4 ONNXRuntime 推理优化加速

本地运行模型时，我们发现相应的 FPS 并不理想，与赛题方所要求的 20FPS 相差甚远，于是我们在部署 ONNXRuntime 推理时，采用了以下的方法进行了优化加速。

（1）使用 opencv 自带的 resize 方法代替设置 downratio 值（即 downratio=1）

因为在 python 中，downratio!=1 时，由 pytorch 中 F.interpolate 的方法先将输入图像进行缩放，而在 ONNXRuntime 中，代替 F.interpolate 的方法仍然没有 cv::resize的方法运行的快，因此在推理过程中只使用 cv::resize 进行一次缩放以提速。

（2）在推理过程中，设置合适的 cpu 推理线程数。根据 onnxruntime 自带的设置线程接口函数，根据 cpu 的实际最大线程数。以 i7-9700k 为例，cpu 的最大线程数为 8，将 onnxruntime 推理线程数设置为 5 或 6 时推理速度最快。

![](https://gitee.com/yao_yi_feng/fighouse/raw/master/img/%E8%A7%86%E9%A2%91%E4%BA%BA%E5%83%8F%E5%88%86%E5%89%B2/202402290004627.webp?width=500#center)

（3）尽可能用第三库的矩阵运算代替自己写的 for 循环遍历。虽然矩阵运算的底层代码也是数组的遍历，但是好的第三方库中（如 opencv 等），将矩阵运算进行了加速处理（例如使用多线程、SIMD 等方式），使得运行速度比自己写的 for 循环快很多。因此在图像的预处理和后处理过程中，尽可能的使用矩阵运算，比如通过 cv::Mat 的构造函数直接将 onnxruntime 的输出 float转化成一个二维图像矩阵。

## 第五章 模型测试

我们在给定的二十个测试集上进行了实例的测试：

具体测试过程为：将测试样例输入到模型中得出对应的人像前景输出与人像 Mask视频，并将手动打标签得到的视频与模型得出的人像 Mask 视频输入到设计好的 MIOU 指标计算程序中计算 MIOU 数值（在计算时我们分别计算了两种不同的 MIOU 数值，分别是包括背景的与去除背景的两种数值）

具体系统测试数据如下：（左一为模型测试效果，左二为人像原图，右一为手动打标签图像）

![image-20240229001845951](https://gitee.com/yao_yi_feng/fighouse/raw/master/img/%E8%A7%86%E9%A2%91%E4%BA%BA%E5%83%8F%E5%88%86%E5%89%B2/202402290020150.webp?width=800#center)
<p align=center>Test 03</p>

![](https://gitee.com/yao_yi_feng/fighouse/raw/master/img/%E8%A7%86%E9%A2%91%E4%BA%BA%E5%83%8F%E5%88%86%E5%89%B2/202402290004595.webp?width=800#center)
<p align=center>Test 10</p>

![image-20240229001902194](https://gitee.com/yao_yi_feng/fighouse/raw/master/img/%E8%A7%86%E9%A2%91%E4%BA%BA%E5%83%8F%E5%88%86%E5%89%B2/202402290020397.webp?width=800#center)
<p align=center>Test 14</p>

本地 MIOU 数值计算：

|测试集序号|MIOU|MIOU with no_back|
|---|---|---|
|Test01|0.9877|0.9798|
|Test02|0.9798|0.9757|
|Test03|0.9733|0.9604|
|Test04|0.9785|0.9693|
|Test05|0.8401|0.6934|
|Test06|0.9878|0.9793|
|Test07|0.9663|0.9378|
|Test08|0.9222|0.8523|
|Test09|0.9207|0.8464|
|Test10|0.9427|0.9052|

可见，我们的模型对于视频人像分割得出了一个令人满意的结果

# 第六章 参考文献资料与引用

1. Liang-Chieh Chen, G. Papandreou, Florian Schroff, and Hartwig Adam. Rethinking atrous convolution for semantic image segmentation. ArXiv, abs/1706.05587, 2017.
    
2. Kyunghyun Cho, B. V. Merrienboer, C¸ aglar Gulc¸ehre, ¨ Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder–decoder for statistical machine translation. ArXiv, abs/1406.1078, 2014.
    
3. Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, Quoc V. Le, and Hartwig Adam. Searching for mobilenetv3, 2019.
    
4. Zhanghan Ke, Kaican Li, Yurou Zhou, Qiuhua Wu, Xiangyu Mao, Qiong Yan, and Rynson W.H. Lau. Is a green screen really necessary for real-time portrait matting? ArXiv,abs/2011.11961, 2020.
    
5. Shanchuan Lin, Andrey Ryabtsev, Soumyadip Sengupta, Brian Curless, Steve Seitz, and Ira KemelmacherShlizerman. Real-time high-resolution background matting. In Computer Vision and Pattern Regognition (CVPR), 2021.
    
6. Soumyadip Sengupta, Vivek Jayaram, Brian Curless, Steve Seitz, and Ira Kemelmacher-Shlizerman. Background matting: The world is your green screen. In Computer Vision and Pattern Regognition (CVPR), 2020.
    
7. Huikai Wu, Shuai Zheng, Junge Zhang, and Kaiqi Huang. Fast end-to-end trainable guided filter, 2019.
    
8. Shanchuan Lin, Linjie Yang, Imran Saleemi and Soumyadip Sengupta.Robust High-Resolution Video Matting with Temporal Guidance. arXiv:2108.11515 [cs.CV]
